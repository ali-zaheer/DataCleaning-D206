{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d7ec3d",
   "metadata": {},
   "source": [
    "# Performance Assessment D206 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce127b62",
   "metadata": {},
   "source": [
    "Ali Zaheer\n",
    "azaheer@wgu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f71d99",
   "metadata": {},
   "source": [
    "### Part I: Research Question\n",
    "\n",
    "###### A.  Describe one question or decision that you will address using the data set you chose. The summarized question or decision must be relevant to a realistic organizational need or situation.\n",
    "\n",
    "Which 'contract' type has high 'churn' and what type of correlation exists in respect to the customer's 'area'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67438289",
   "metadata": {},
   "source": [
    "###### B.  Describe the variables in the data set and indicate the specific type of data being described. Use examples from the data set that support your claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "df = pd.read_csv('dataSet/churn_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6801aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the data set\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and their data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16032e",
   "metadata": {},
   "source": [
    "### Part II: Data-Cleaning Plan\n",
    "\n",
    "###### C.  Explain the plan for cleaning the data by doing the following:\n",
    "1.  Propose a plan that includes the relevant techniques and specific steps needed to identify anomalies in the data set.\n",
    "    \n",
    "    1. Use Pandas to import the CSV file in the data frame.\n",
    "    2. Examine and ensure data type consistency in the columns.\n",
    "    3. Validate that each column has the same data type.\n",
    "    4. Identify and resolve spelling mistakes in column headers or row level data.\n",
    "    5. Identify and remove outliers\n",
    "        - Outliers are identified using Z-score and boxplot graphs.\n",
    "        - Validate if the outliers are to be removed or kept\n",
    "    6. Identify, Standardize and replaced missing values using central tendency (Mean, Mode or Median)\n",
    "    \n",
    "    (Larose, 2019, p.29-43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8a91f",
   "metadata": {},
   "source": [
    "###### 2.  Justify your approach for assessing the quality of the data, include:\n",
    "###### characteristics of the data being assessed:\n",
    "\n",
    "There are 10,000 customer related records with 52 related variables in this data set. The 'Churn' column describes and defines whether the customer has cancelled their service(s) in last month.\n",
    "\n",
    "Other variables that are related to each customer are categorically captured below:\n",
    "\n",
    "- Services that each customer has signed up for (phone, multiple lines, internet, online security, online backup, device protection, technical support, and streaming TV and movies)\n",
    "- Customer account related information (how long they’ve been a customer, contracts, payment methods, paperless billing, monthly charges, GB usage over a year, etc.)\n",
    "- Customer demographics (gender, age, job, income, etc.)\n",
    "\n",
    "##### Approach used to assess the quality:\n",
    "- Validate each column to ensure its data is consistent with its data type.\n",
    "- Identify and resolve spelling mistakes in column headers.\n",
    "- Identify and remove outliers.\n",
    "    - Outliers are identified using Z-score and/or boxplot graphs.\n",
    "- Identify and replace missing values using central tendency (Median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f8bb7",
   "metadata": {},
   "source": [
    "###### 3.  Justify your selected programming language and any libraries and packages that will support the data-cleaning process.\n",
    "    A.I will utilize Python due to my previous interaction with it and its Pandas, matplotlib and Scipy modules. Additionally, I will be using Jupyter notebook as the IDE because it provides a user-friendly experience.\n",
    "    Pandas is an excellent package for working with data set as it makes it easy to load and manipulate columns and/or rows to replace null values. \n",
    "    Matplotlib plot is an easy way to create graphs for identifying outliers using histograms and/or boxplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485893d6",
   "metadata": {},
   "source": [
    "##### 4.  Provide the code you will use to identify the anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615409a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "df = pd.read_csv('dataSet/churn_raw_data.csv', dtype={'CaseOrder':np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c1ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display data set with all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the data set\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdaed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and their data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e930b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column with no headers\n",
    "df = df.drop(df.columns[[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0299345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amend columns with no names\n",
    "df = df.rename(columns=({ 'item1': 'Timely response', 'item2':'Timely fixes', 'item3':'Timely replacements', \n",
    "                         'item4':'Reliability', 'item5':'Options', 'item6':'Respectful response',\n",
    "                         'item7':'Courteous exchange', 'item8':'Evidence of active listening'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488bd76",
   "metadata": {},
   "source": [
    "#### Identify spelling mistakes in the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in Area column\n",
    "df['Area'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in Employment column\n",
    "df['Employment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in Gender column\n",
    "df['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62589bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in Marital column\n",
    "df['Marital'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in PaymentMethod column\n",
    "df['PaymentMethod'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b706db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in InternetService column\n",
    "df['InternetService'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique data in Job column\n",
    "df['Job'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a7ec1",
   "metadata": {},
   "source": [
    "#### Reexpression of categorical data as numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a71bb2",
   "metadata": {},
   "source": [
    "#### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87171ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture unique values from the 'Education' column for Re-Expression\n",
    "df['Education'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce024de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-expression categorial data in 'Education' columns\n",
    "dict_edu= {'Education': {\n",
    "     'No Schooling Completed': 0,\n",
    "     'Nursery School to 8th Grade': 8,\n",
    "     '9th Grade to 12th Grade, No Diploma':11,\n",
    "     'Regular High School Diploma': 12,\n",
    "     'GED or Alternative Credential': 12,\n",
    "     'Some College, Less than 1 Year': 12,\n",
    "     'Some College, 1 or More Years, No Degree': 12,\n",
    "     'Professional School Degree':13,\n",
    "     \"Associate's Degree\": 14,\n",
    "     \"Bachelor's Degree\": 16,\n",
    "     \"Master's Degree\": 18,\n",
    "     'Doctorate Degree': 20,\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Reexpression values\n",
    "df.replace(dict_edu, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display data set with Re-Expressioned 'Education' column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21479fdf",
   "metadata": {},
   "source": [
    "#### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and isolate the columns with null \n",
    "df.loc[:,df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ecdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of missing values per columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb346d",
   "metadata": {},
   "source": [
    "#### Change Misleading Field Values\n",
    "##### Limitations: replacing missing value can cause the data set to be inflated as I am trying to impose what could be the accurate value\n",
    "* Children: Customer might have chosen not to tell the actual number of children they have due to privacy concerns\n",
    "* Phone: Customer might have chosen not to list their phone number due to privacy concerns.\n",
    "* Techie: This could have been left out a human error.\n",
    "* TechSupport: This could a human error, someone might not have entered appropriate values assuming 'No' and '' are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NAN in Childern column with 0 as it already has 0 value for people with no childern.\n",
    "df['Children']=df['Children'].replace({np.NaN:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NAN in Phone column with No, as either a person has a phone or they do not\n",
    "df['Phone']=df['Phone'].replace({np.NaN:\"No\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NAN in Techie column with No\n",
    "df['Techie']=df['Techie'].replace({np.NaN:\"No\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a901366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NAN in TechSupport column with No\n",
    "df['TechSupport']=df['TechSupport'].replace({np.NaN:\"No\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5493f",
   "metadata": {},
   "source": [
    "#### Identify Missing Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfebb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data in Age column\n",
    "df[\"Age\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dfbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list out all values including null\n",
    "df[\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Age distribution\n",
    "df[\"Age\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc26761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data in Income column\n",
    "df[\"Income\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list out all values including null\n",
    "df[\"Income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Income distribution\n",
    "df[\"Income\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db98913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data in Tenure column\n",
    "df[\"Tenure\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data in Tenure column\n",
    "df[\"Tenure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Tenure distribution\n",
    "df[\"Tenure\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data in Bandwidth_GB_Year column\n",
    "df[\"Bandwidth_GB_Year\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data in Bandwidth_GB_Year column\n",
    "df[\"Bandwidth_GB_Year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Bandwidth_GB_Year distribution\n",
    "df[\"Bandwidth_GB_Year\"].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69344a89",
   "metadata": {},
   "source": [
    "#### Replace Missing Numeric Values with Median becuase the distrution of data is skewed as displayed above.\n",
    "##### This is a robust measure that is not strongly influenced by the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the NAN in age with median\n",
    "df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the NAN in income with median\n",
    "df[\"Income\"].fillna(df[\"Income\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the NAN in Tenure with median\n",
    "df[\"Tenure\"].fillna(df[\"Tenure\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the NAN in Bandwidth_GB_Year with median\n",
    "df[\"Bandwidth_GB_Year\"].fillna(df[\"Bandwidth_GB_Year\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b905484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate all the null values have been replaced \n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a06eee",
   "metadata": {},
   "source": [
    "### Cleaned data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd81b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned data set\n",
    "df.to_csv('Cleaned_Data_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78f801",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sns settings \n",
    "sns.set(rc={'figure.figsize':(30,11)}, font_scale=1.5, style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look to see which columns have outliers\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63890baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of all the columns with outliers\n",
    "df.boxplot(['Population', 'Income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b218674",
   "metadata": {},
   "source": [
    "##### Investigate Outliers in the Income column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using box plot plot to identify outliers\n",
    "Income = df['Income']\n",
    "Income.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate distribution of Income column using histogram\n",
    "df[\"Income\"].plot(kind = \"hist\", title = 'Income Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfac288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with standarized Income values\n",
    "df[\"Income_z\"] = stats.zscore(df[\"Income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the z score isolate the outliers\n",
    "df_income_outliers = df.query('Income_z > 3 | Income_z < -3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb21b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set for the outliers and sort it in descending order\n",
    "df_income_outliers_sort_values = df_income_outliers.sort_values(['Income_z'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out the outliers\n",
    "df_income_outliers_sort_values['Income'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c870f",
   "metadata": {},
   "source": [
    "##### Investigate Outliers in the Population column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using box plot plot to identify outliers\n",
    "Population = df['Population']\n",
    "Population.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81495a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate distribution of Population column using histogram\n",
    "df[\"Population\"].plot(kind = \"hist\", title = 'Population Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with standarized median values\n",
    "df[\"Population_z\"] = stats.zscore(df[\"Population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the z score isolate the outliers\n",
    "df_Population_outliers = df.query('Population_z > 3 | Population_z < -3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set for the outliers and sort it in descending order\n",
    "df_Population_outliers_sort_values = df_Population_outliers.sort_values(['Population_z'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out the outliers\n",
    "df_Population_outliers_sort_values['Population'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f24400",
   "metadata": {},
   "source": [
    "#### PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data frame\n",
    "df = pd.read_csv('dataSet/churn_raw_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick view of the data-set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957932aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add names to the customer feedback columns\n",
    "df = df.rename(columns=({ 'item1': 'Timely response', 'item2':'Timely fixes', 'item3':'Timely replacements', \n",
    "                         'item4':'Reliability', 'item5':'Options', 'item6':'Respectful response',\n",
    "                         'item7':'Courteous exchange', 'item8':'Evidence of active listening'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA analysis data-set with feedback response\n",
    "customer_data = df[['Timely response', 'Timely fixes', 'Timely replacements', 'Reliability','Options','Respectful response',\n",
    "                         'Courteous exchange','Evidence of active listening']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data frame\n",
    "customer_data_norm = (customer_data-customer_data.mean())/customer_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component extraction\n",
    "pca= PCA(n_components=customer_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA fitting\n",
    "pca.fit(customer_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269410af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA transform and normalization\n",
    "customer_pca = pd.DataFrame(pca.transform(customer_data_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3827b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle Component for the Scree plot\n",
    "columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot showing the PCs\n",
    "# Below show the 60 percent of the variance is explained by 2 component \n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance (%)',  fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues\n",
    "cov_matrix = np.dot(customer_data_norm.T, customer_data_norm) / customer_data.shape[0]\n",
    "EigenV = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot show Eigen Values\n",
    "# PC0 and PC1 has Eigenvalues greated than 1.\n",
    "plt.plot(EigenV)\n",
    "plt.xlabel('Pricipal Components')\n",
    "plt.ylabel('Eigen Values')\n",
    "plt.title('Eigen Values',  fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and identifying the PC from the Customer dataframe\n",
    "loading = pd.DataFrame(pca.components_.T, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8'], index=customer_data.columns)\n",
    "loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate and show values of the PC1\n",
    "load = loading['PC1'] > .4\n",
    "loading[load]['PC1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2272a5",
   "metadata": {},
   "source": [
    "### Part III: Data Cleaning\n",
    "\n",
    "###### D.  Summarize the data-cleaning process by doing the following:\n",
    "D1. I was able to find 8 columns with anomalies. Children, Phone, Techie and TechSupport were categorical, and their ‘null’ values were replaced with ‘No’. I used the 'Reexpression of Categorial column' to create the Education column. The limitations are as follow:\n",
    "\n",
    "D2. Categorical data imputation limitation can distort the data if the assumptions are not confirmed.\n",
    "* Children: The customer might have chosen not to tell the actual number of children they have due to privacy concerns.\n",
    "* Phone: Customer might have chosen not to list their phone number due to privacy concerns.\n",
    "* Techie: This could have been left out as a human error.\n",
    "* TechSupport: This could a human error, someone might not have entered appropriate values assuming 'No' and '' are the same.\n",
    "\n",
    "D2a. Numerical data\n",
    "The continuous type columns (Age, Income, Tenure, Bandwidth_GB_Year) data were replaced using python’s median functions because these are continuous and the data was either skewed to left or Bimodal. I chose this because it is simple, easy to apply method and does not reduce the sample size. on the limitation side, it is possible to distort data / distribution of the data. The rest of the columns were not part of the process as they did not have any null values.\n",
    "\n",
    "D3. All the missing categorical values were imputed to ‘No’ and numerical values were imputed using median central tendency. Age and Tenure were left alone as they did not have any outliers.\n",
    "\n",
    "D4. Code is available above and in the Panopto recording\n",
    "\n",
    "D5. Attached file ‘Cleaned_Data_set.csv’) \n",
    "\n",
    "D6 & D7. The data cleaning process assumes that replacing categorical null values with ‘No’ is the right approach however this can lead to inflated data that will lean toward replaced values and can lead to inaccurate decision making. Similarly, using statical central tendencies is an appropriate approach but can lead to inflated data and imbalanced decision making. Imputing data values using the above steps can give us a picture but cannot replace true values which were missed due to human error/system errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bce2d9",
   "metadata": {},
   "source": [
    "###### E.  Apply principal component analysis (PCA) to identify the significant features of the data set by doing the following:\n",
    "1.  List the principal components in the data set.\n",
    "* Timely response\n",
    "* Timely fixes\n",
    "* Timely replacements\n",
    "* Respectful response\n",
    "2.  Describe how you identified the principal components of the data set.\n",
    "* PC0 and PC1 should be kept as they have Eigenvalues greater than 1.\n",
    "3.  Describe how the organization can benefit from the results of the PCA\n",
    "* The four identified scores should be reviewed carefully to understand customer's feedback. This will help the company to keep their customer for a longer time hence increasing profits.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7b06c",
   "metadata": {},
   "source": [
    "### Part IV. Supporting Documents\n",
    "\n",
    "###### F.  Provide a Panopto recording that demonstrates the warning- and error-free functionality of the code used to support the discovery of anomalies and the data cleaning process and summarizes the programming environment.\n",
    " \n",
    "Note: For instructions on how to access and use Panopto, use the \"Panopto How-To Videos\" web link provided below. To access Panopto's website, navigate to the web link titled \"Panopto Access\", and then choose to log in using the “WGU” option. If prompted, log in using your WGU student portal credentials, and then it will forward you to Panopto’s website.\n",
    " \n",
    "To submit your recording, upload it to the Panopto drop box titled “Data Cleaning – NUM2 \\ D206” Once the recording has been uploaded and processed in Panopto's system, retrieve the URL of the recording from Panopto and copy and paste it into the Links option. Upload the remaining task requirements using the Attachments option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab4be7",
   "metadata": {},
   "source": [
    "###### G.  Reference the web sources used to acquire segments of third-party code to support the application. Be sure the web sources are reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88621d",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "Pandas. (2021). Pandas DataFrames. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html\n",
    "\n",
    "Get started with references. (2021). Jupyterbook. https://jupyterbook.org/tutorials/references.html#tutorials-references\n",
    "\n",
    "Marques, A. M. (2020, March 11). How to show all columns / rows of a Pandas Dataframe? Towards Data Science. https://towardsdatascience.com/how-to-show-all-columns-rows-of-a-pandas-dataframe-c49d4507fcf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686bd140",
   "metadata": {},
   "source": [
    "###### H.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.\n",
    "```{bibliography}\n",
    "Chantal D. Larose, & Daniel T. Larose. (2019). Data Science Using Python and R. Wiley.\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
